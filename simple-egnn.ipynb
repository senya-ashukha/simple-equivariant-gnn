{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6a5090",
   "metadata": {},
   "source": [
    "# Simple Impementation of E(n) Equivariant Graph Neural Networks\n",
    "\n",
    "Original paper https://arxiv.org/pdf/2102.09844.pdf by Victor Garcia Satorras, Emiel Hoogeboom, Max Welling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bU4ixrOJCg1",
   "metadata": {
    "id": "4bU4ixrOJCg1"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb08a10",
   "metadata": {},
   "source": [
    "# Load QM9 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859f981c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "859f981c",
    "outputId": "3b62e11b-79be-4cbd-f9ff-38ccc05d013b"
   },
   "outputs": [],
   "source": [
    "# QM9 is a dataset for Molecular Property Predictions http://quantum-machine.org/datasets/\n",
    "# We will predict Highest occupied molecular orbital energy \n",
    "# https://en.wikipedia.org/wiki/HOMO_and_LUMO\n",
    "# We use data loaders from the official repo\n",
    "\n",
    "from qm9.data_utils import get_data, BatchGraph\n",
    "train_loader, val_loader, test_loader, charge_scale = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e20004",
   "metadata": {},
   "source": [
    "# Graph Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0acbcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In the batch: num_graphs 96 num_nodes 1759\n",
       "> .h \t\t a tensor of nodes representations \t\tshape 1759 x 15\n",
       "> .x \t\t a tensor of nodes positions  \t\t\tshape 1759 x 3\n",
       "> .edges \t a tensor of edges, a fully connected graph \tshape 31288 x 2\n",
       "> .batch  \t a tensor of graph_ids for each node \t\ttensor([ 0,  0,  0,  ..., 95, 95, 95])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = BatchGraph(iter(train_loader).next(), False, charge_scale)\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784c0726",
   "metadata": {},
   "source": [
    "# Define Equivariant Graph Convs  & GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e5e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_sum(agg_size, source, idx, cuda):\n",
    "    \"\"\"\n",
    "        source is N x hid_dim [float]\n",
    "        idx    is N           [int]\n",
    "        \n",
    "        Sums the rows source[.] with the same idx[.];\n",
    "    \"\"\"\n",
    "    tmp = torch.zeros((agg_size, source.shape[1]))\n",
    "    tmp = tmp.cuda() if cuda else tmp\n",
    "    res = torch.index_add(tmp, 0, idx, source)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d5d55db",
   "metadata": {
    "id": "4d5d55db"
   },
   "outputs": [],
   "source": [
    "class ConvEGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, cuda=True):\n",
    "        super().__init__()\n",
    "        self.hid_dim=hid_dim\n",
    "        self.cuda = cuda\n",
    "        \n",
    "        # computes messages based on hidden representations -> [0, 1]\n",
    "        self.f_e = nn.Sequential(\n",
    "            nn.Linear(in_dim*2+1, hid_dim), nn.SiLU(),\n",
    "            nn.Linear(hid_dim, hid_dim), nn.SiLU())\n",
    "        \n",
    "        # preducts \"soft\" edges based on messages \n",
    "        self.f_inf = nn.Sequential( \n",
    "            nn.Linear(hid_dim, 1),\n",
    "            nn.Sigmoid()) \n",
    "        \n",
    "        # updates hidden representations -> [0, 1]\n",
    "        self.f_h = nn.Sequential(\n",
    "            nn.Linear(in_dim+hid_dim, hid_dim), nn.SiLU(),\n",
    "            nn.Linear(hid_dim, hid_dim))\n",
    "    \n",
    "    def forward(self, b):\n",
    "        e_st, e_end = b.edges[:,0], b.edges[:,1]\n",
    "        dists = torch.norm(b.x[e_st] - b.x[e_end], dim=1).reshape(-1, 1)\n",
    "        \n",
    "        # compute messages\n",
    "        tmp = torch.hstack([b.h[e_st], b.h[e_end], dists])\n",
    "        m_ij = self.f_e(tmp)\n",
    "        \n",
    "        # predict edges\n",
    "        e_ij = self.f_inf(m_ij)\n",
    "        \n",
    "        # average e_ij-weighted messages  \n",
    "        # m_i is num_nodes x hid_dim\n",
    "        m_i = index_sum(b.h.shape[0], e_ij*m_ij, b.edges[:,0], self.cuda)\n",
    "        \n",
    "        # update hidden representations\n",
    "        b.h += self.f_h(torch.hstack([b.h, m_i]))\n",
    "\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10aad7c4",
   "metadata": {
    "id": "10aad7c4"
   },
   "outputs": [],
   "source": [
    "class NetEGNN(nn.Module):\n",
    "    def __init__(self, in_dim=15, hid_dim=128, out_dim=1, n_layers=7, cuda=True):\n",
    "        super().__init__()\n",
    "        self.hid_dim=hid_dim\n",
    "        \n",
    "        self.emb = nn.Linear(in_dim, hid_dim) \n",
    "        \n",
    "        n_layers = 7\n",
    "        self.gnn = [ConvEGNN(hid_dim, hid_dim, cuda=cuda) for _ in range(n_layers)]\n",
    "        self.gnn = nn.Sequential(*self.gnn)\n",
    "        \n",
    "        self.pre_mlp = nn.Sequential(\n",
    "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
    "            nn.Linear(hid_dim, hid_dim))\n",
    "        \n",
    "        self.post_mlp = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
    "            nn.Linear(hid_dim, out_dim))\n",
    "\n",
    "        if cuda: self.cuda()\n",
    "        self.cuda = cuda\n",
    "    \n",
    "    def forward(self, b):\n",
    "        b.h = self.emb(b.h)\n",
    "        \n",
    "        b = self.gnn(b)\n",
    "        h_nodes = self.pre_mlp(b.h)\n",
    "        \n",
    "        # h_graph is num_graphs x hid_dim\n",
    "        h_graph = index_sum(b.nG, h_nodes, b.batch, self.cuda) \n",
    "        \n",
    "        out = self.post_mlp(h_graph)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7f4cef6",
   "metadata": {
    "id": "b7f4cef6"
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "cuda = True\n",
    "\n",
    "model = NetEGNN(n_layers=7, cuda=cuda)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-16)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d6b1c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3613c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de3613c9",
    "outputId": "a924add2-aadc-4669-e7cb-3a92c13ba5fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> start training\n",
      "> epoch 000: "
     ]
    }
   ],
   "source": [
    "print('> start training')\n",
    "\n",
    "tr_ys = train_loader.dataset.data['homo'] \n",
    "me, mad = torch.mean(tr_ys), torch.mean(torch.abs(tr_ys - torch.mean(tr_ys)))\n",
    "\n",
    "if cuda:\n",
    "    me = me.cuda()\n",
    "    mad = mad.cuda()\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('> epoch %s:' % str(epoch).zfill(3), end=' ', flush=True) \n",
    "    start = time.time()\n",
    "\n",
    "    batch_train_loss = []\n",
    "    batch_val_loss = []\n",
    "    batch_test_loss = []\n",
    "\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch = BatchGraph(batch, cuda, charge_scale)\n",
    "        \n",
    "        out = model(batch).reshape(-1)\n",
    "        loss =  F.l1_loss(out,  (batch.y-me)/mad)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss =  F.l1_loss(out*mad+me, batch.y)\n",
    "\n",
    "        batch_train_loss += [float(loss.data.cpu().numpy())]  \n",
    "        \n",
    "    train_loss += [np.mean(batch_train_loss)/0.001]\n",
    "    \n",
    "    print('train %.3f' % train_loss[-1], end=' ', flush=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in val_loader:\n",
    "            batch = BatchGraph(batch, cuda, charge_scale)\n",
    "            out = model(batch).reshape(-1)\n",
    "            loss = F.l1_loss(out*mad+me, batch.y).data.cpu().numpy()\n",
    "            batch_val_loss += [np.mean(loss)]\n",
    "            \n",
    "        val_loss += [np.mean(batch_val_loss)/0.001]\n",
    "        \n",
    "        print('val %.3f' % val_loss[-1], end=' ', flush=True)\n",
    "        \n",
    "        for batch in test_loader:\n",
    "            batch = BatchGraph(batch, cuda, charge_scale)\n",
    "            out = model(batch).reshape(-1)\n",
    "            loss = F.l1_loss(out*mad+me, batch.y).data.cpu().numpy()\n",
    "            batch_test_loss += [np.mean(loss)]\n",
    "\n",
    "        test_loss += [np.mean(batch_test_loss)/0.001]\n",
    "        \n",
    "    end = time.time()\n",
    "\n",
    "    print('test %.3f (%.1f sec)' % (test_loss[-1], end-start), flush=True)\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7825a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# > start training \n",
    "# > epoch 000: train 264.008 val 243.351 test 242.635 (94.3 sec)\n",
    "# > epoch 001: train 211.893 val 211.575 test 210.144 (92.8 sec)\n",
    "# > epoch 002: train 185.362 val 164.960 test 165.087 (93.9 sec)\n",
    "# > epoch 003: train 163.121 val 150.953 test 150.533 (93.2 sec)\n",
    "# ...\n",
    "# > epoch 998: train 0.032 val 30.157 test 30.886 (93.4 sec)\n",
    "# > epoch 999: train 0.032 val 30.157 test 30.886 (93.4 sec)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "graph_seminar_homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
